import unittest
from tokenizer.tokenizer import tokenize


class TokenizerIndustrialTests(unittest.TestCase):

    def test_basic_persian_sentence(self):
        tokens = tokenize("Ø³Ù„Ø§Ù…ØŒ Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ")
        self.assertEqual(tokens, ["Ø³Ù„Ø§Ù…", "ØŒ", "Ø­Ø§Ù„Øª", "Ú†Ø·ÙˆØ±Ù‡", "ØŸ"])

    def test_no_empty_tokens(self):
        tokens = tokenize("   Ø³Ù„Ø§Ù…    Ø¯Ù†ÛŒØ§   ")
        self.assertTrue(all(token.strip() for token in tokens))

    def test_repeated_char_normalization(self):
        tokens = tokenize("Ø³Ù„Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ø§Ù…")
        self.assertIn("Ø³Ù„Ø§Ù…", tokens)

    def test_punctuation_isolated(self):
        tokens = tokenize("Ø³Ù„Ø§Ù…!!!Ø®ÙˆØ¨ÛŒØŸØŸ")
        self.assertIn("!", tokens)
        self.assertIn("ØŸ", tokens)
        self.assertNotIn("!!!", tokens)

    def test_unicode_half_space(self):
        tokens = tokenize("Ù…ÛŒ\u200cØ±ÙˆÙ…")
        self.assertIn("Ù…ÛŒØ±ÙˆÙ…", tokens)

    def test_tatweel_removed(self):
        tokens = tokenize("Ú†ÛŒÙ€Ù‡")
        self.assertIn("Ú†ÛŒÙ‡", tokens)

    def test_html_removed(self):
        tokens = tokenize("<b>Ø³Ù„Ø§Ù…</b>")
        self.assertEqual(tokens, ["Ø³Ù„Ø§Ù…"])

    def test_url_removed(self):
        tokens = tokenize("Ø³Ù„Ø§Ù… https://test.com")
        self.assertEqual(tokens, ["Ø³Ù„Ø§Ù…"])

    def test_emoji_removed(self):
        tokens = tokenize("Ø³Ù„Ø§Ù… ðŸ˜‚ðŸ˜‚")
        self.assertEqual(tokens, ["Ø³Ù„Ø§Ù…"])

    def test_deterministic(self):
        text = "Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§"
        self.assertEqual(tokenize(text), tokenize(text))

    def test_mixed_language(self):
        tokens = tokenize("Ø³Ù„Ø§Ù… hello Ø¯Ù†ÛŒØ§ 123")
        self.assertIn("Ø³Ù„Ø§Ù…", tokens)
        self.assertIn("hello", tokens)

    def test_no_whitespace_inside_token(self):
        tokens = tokenize("Ø³Ù„Ø§Ù… Ø¯Ù†ÛŒØ§")
        for t in tokens:
            self.assertNotRegex(t, r"\s")


if __name__ == "__main__":
    unittest.main()
